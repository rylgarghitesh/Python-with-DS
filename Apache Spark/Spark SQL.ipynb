{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b41a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c67ab9e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e508bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(id=int(fields[0]), name=str(fields[1]), \\\n",
    "               age=int(fields[2]), numFriends=int(fields[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7a4ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/17 18:20:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f58c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.textFile(\"fakefriends.csv\")\n",
    "people = lines.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d43876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acd5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c83d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=21, name='Miles', age=19, numFriends=268)\n",
      "Row(id=52, name='Beverly', age=19, numFriends=269)\n",
      "Row(id=54, name='Brunt', age=19, numFriends=5)\n",
      "Row(id=106, name='Beverly', age=18, numFriends=499)\n",
      "Row(id=115, name='Dukat', age=18, numFriends=397)\n",
      "Row(id=133, name='Quark', age=19, numFriends=265)\n",
      "Row(id=136, name='Will', age=19, numFriends=335)\n",
      "Row(id=225, name='Elim', age=19, numFriends=106)\n",
      "Row(id=304, name='Will', age=19, numFriends=404)\n",
      "Row(id=341, name='Data', age=18, numFriends=326)\n",
      "Row(id=366, name='Keiko', age=19, numFriends=119)\n",
      "Row(id=373, name='Quark', age=19, numFriends=272)\n",
      "Row(id=377, name='Beverly', age=18, numFriends=418)\n",
      "Row(id=404, name='Kasidy', age=18, numFriends=24)\n",
      "Row(id=409, name='Nog', age=19, numFriends=267)\n",
      "Row(id=439, name='Data', age=18, numFriends=417)\n",
      "Row(id=444, name='Keiko', age=18, numFriends=472)\n",
      "Row(id=492, name='Dukat', age=19, numFriends=36)\n",
      "Row(id=494, name='Kasidy', age=18, numFriends=194)\n"
     ]
    }
   ],
   "source": [
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "for teen in teenagers.collect():\n",
    "  print(teen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c874229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 18|    8|\n",
      "| 19|   11|\n",
      "| 20|    5|\n",
      "| 21|    8|\n",
      "| 22|    7|\n",
      "| 23|   10|\n",
      "| 24|    5|\n",
      "| 25|   11|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 28|   10|\n",
      "| 29|   12|\n",
      "| 30|   11|\n",
      "| 31|    8|\n",
      "| 32|   11|\n",
      "| 33|   12|\n",
      "| 34|    6|\n",
      "| 35|    8|\n",
      "| 36|   10|\n",
      "| 37|    9|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also use functions instead of SQL queries:\n",
    "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17b604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99bdb1",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e270f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8238145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF created with schema\n",
    "people = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc6c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: integer (nullable = true)\n",
      " |-- Will: string (nullable = true)\n",
      " |-- 33: integer (nullable = true)\n",
      " |-- 385: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e29ac965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[0: int, Will: string, 33: int, 385: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.sample(fraction=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5077b",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aabe3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.textFile(\"fakefriends.csv\")\n",
    "people = lines.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7267f144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43c0bcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint, numFriends: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bad828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = schemaPeople.select(schemaPeople.age, schemaPeople.numFriends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e585f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[age: bigint, numFriends: bigint]\n"
     ]
    }
   ],
   "source": [
    "print(friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0c26138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|age|   avg(numFriends)|\n",
      "+---+------------------+\n",
      "| 26|242.05882352941177|\n",
      "| 29|215.91666666666666|\n",
      "| 65|             298.2|\n",
      "| 54| 278.0769230769231|\n",
      "| 19|213.27272727272728|\n",
      "| 22|206.42857142857142|\n",
      "| 34|             245.5|\n",
      "| 50|             254.6|\n",
      "| 57| 258.8333333333333|\n",
      "| 43|230.57142857142858|\n",
      "| 32| 207.9090909090909|\n",
      "| 31|            267.25|\n",
      "| 39|169.28571428571428|\n",
      "| 25|197.45454545454547|\n",
      "| 68|             269.6|\n",
      "| 58|116.54545454545455|\n",
      "| 27|           228.125|\n",
      "| 63|             384.0|\n",
      "| 56| 306.6666666666667|\n",
      "| 51|302.14285714285717|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends.groupBy(\"age\").avg(\"numFriends\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "072b1bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|age|   avg(numFriends)|\n",
      "+---+------------------+\n",
      "| 18|           343.375|\n",
      "| 19|213.27272727272728|\n",
      "| 20|             165.0|\n",
      "| 21|           350.875|\n",
      "| 22|206.42857142857142|\n",
      "| 23|             246.3|\n",
      "| 24|             233.8|\n",
      "| 25|197.45454545454547|\n",
      "| 26|242.05882352941177|\n",
      "| 27|           228.125|\n",
      "| 28|             209.1|\n",
      "| 29|215.91666666666666|\n",
      "| 30| 235.8181818181818|\n",
      "| 31|            267.25|\n",
      "| 32| 207.9090909090909|\n",
      "| 33| 325.3333333333333|\n",
      "| 34|             245.5|\n",
      "| 35|           211.625|\n",
      "| 36|             246.6|\n",
      "| 37|249.33333333333334|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends.groupBy(\"age\").avg(\"numFriends\").sort(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65d18ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|age|avg of friends|\n",
      "+---+--------------+\n",
      "| 18|        343.38|\n",
      "| 19|        213.27|\n",
      "| 20|         165.0|\n",
      "| 21|        350.88|\n",
      "| 22|        206.43|\n",
      "| 23|         246.3|\n",
      "| 24|         233.8|\n",
      "| 25|        197.45|\n",
      "| 26|        242.06|\n",
      "| 27|        228.13|\n",
      "| 28|         209.1|\n",
      "| 29|        215.92|\n",
      "| 30|        235.82|\n",
      "| 31|        267.25|\n",
      "| 32|        207.91|\n",
      "| 33|        325.33|\n",
      "| 34|         245.5|\n",
      "| 35|        211.63|\n",
      "| 36|         246.6|\n",
      "| 37|        249.33|\n",
      "+---+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends.groupBy(\"age\").agg(func.round(func.avg(\"numFriends\"),2).alias(\"avg of friends\")).sort(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc9cc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6e445c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4db32d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.square(x)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"square\", square, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc44b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.sql(\"select square(age) from friends.show()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5d9ac",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00598ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDf = spark.read.text('book.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4217f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = inputDf.select(func.explode(func.split(inputDf.value, '\\\\W+')).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e0fe31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.filter(words.word != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce451e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = words.select(func.lower(words.word).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3868e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word = lower.groupBy(\"word\").count().sort(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00432899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|          125|    1|\n",
      "| manipulation|    1|\n",
      "|       graphs|    1|\n",
      "|indoctrinated|    1|\n",
      "|       column|    1|\n",
      "|    traveling|    1|\n",
      "|     slightly|    1|\n",
      "| inflammatory|    1|\n",
      "|   variations|    1|\n",
      "|       spared|    1|\n",
      "|          800|    1|\n",
      "|    indicator|    1|\n",
      "|        hires|    1|\n",
      "|           07|    1|\n",
      "|   surrounded|    1|\n",
      "|     retailer|    1|\n",
      "|          fax|    1|\n",
      "|   afterwards|    1|\n",
      "|        boost|    1|\n",
      "|    directors|    1|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_word.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc622286",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67e305eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d248adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", IntegerType(), True),\n",
    "    StructField(\"measure\", StringType(), True),\n",
    "    StructField(\"temp\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d486bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).csv(\"1800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e5d191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1fae81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minTemp = df.filter(df.measure == \"TMIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47c5f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationTemp = minTemp.select(\"id\", \"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e45a0e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|         id|min(temp)|\n",
      "+-----------+---------+\n",
      "|ITE00100554|   -148.0|\n",
      "|EZE00100082|   -135.0|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stationTemp.groupBy(\"id\").min(\"temp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "545abd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f205881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
