{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1843247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64a335",
   "metadata": {},
   "source": [
    "# ParDo Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33fd4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitRow(beam.DoFn):\n",
    "    \n",
    "    def process(self, element):\n",
    "        return [element.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92a88926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterRow(beam.DoFn):\n",
    "    \n",
    "    def process(self, element) :\n",
    "        if element[3] == \"Accounts\" :\n",
    "            return [element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "636c6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairRow(beam.DoFn):\n",
    "    \n",
    "    def process(self, element) :\n",
    "        return [(element[1],1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a06253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counting(beam.DoFn):\n",
    "    \n",
    "    def process(self, element) :\n",
    "        (key, values) = element\n",
    "        return [(key, sum(values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b66c0371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco', 31)\r\n",
      "('Rebekah', 31)\r\n",
      "('Itoe', 31)\r\n",
      "('Edouard', 31)\r\n",
      "('Kyle', 62)\r\n",
      "('Kumiko', 31)\r\n",
      "('Gaston', 31)\r\n",
      "('Ayumi', 30)\r\n"
     ]
    }
   ],
   "source": [
    "p1 = beam.Pipeline()\n",
    "attendence_count = (\n",
    "    p1 \n",
    "    | beam.io.ReadFromText('dept_data.txt')\n",
    "    | beam.ParDo(SplitRow())\n",
    "#     | beam.ParDo(lambda element: [element.split(',')])\n",
    "    | beam.ParDo(FilterRow())\n",
    "    | beam.ParDo(PairRow())\n",
    "    | beam.GroupByKey()\n",
    "    | beam.ParDo(Counting())\n",
    "    | beam.io.WriteToText('data/output')       \n",
    ")\n",
    "\n",
    "p1.run()\n",
    "\n",
    "!{'head -n 20 data/output-00000-of-00001'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055afc1",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d70b83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return (0.0,0)\n",
    "    \n",
    "    def add_input(self, sum_count, inp):\n",
    "        (sums, count) = sum_count\n",
    "        return sums+inp, count+1\n",
    "        \n",
    "    def merge_accumulators(self, accum):\n",
    "        ind_sum, ind_count = zip(*accum)\n",
    "        return sum(ind_sum), sum(ind_count)\n",
    "        \n",
    "    def extract_output(self,sum_count):\n",
    "        (sums,count) = sum_count\n",
    "        return sums/count if count else float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f6d39ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5\r\n"
     ]
    }
   ],
   "source": [
    "p2 = beam.Pipeline()\n",
    "\n",
    "small_count = (\n",
    "    p2 \n",
    "    | beam.Create([15,5,7,7,9,23,13,5])\n",
    "    | beam.CombineGlobally(AverageFn())\n",
    "    | beam.io.WriteToText('data/output')       \n",
    ")\n",
    "\n",
    "p2.run()\n",
    "\n",
    "!{'head -n 20 data/output-00000-of-00001'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e4fd4",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22dcde",
   "metadata": {},
   "source": [
    "# Composite Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7f05613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_on_count(element):\n",
    "    name, count = element\n",
    "    if count > 30: \n",
    "        return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0b4644ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(element):\n",
    "    name, count = element\n",
    "    return name + \", \" + str(count) + \", Regular Employee.\" \n",
    "# .join((name.encode('ascii')), str(count), \"Regular Employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cef3c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransform(beam.PTransform):\n",
    "    \n",
    "    def expand(self, input_col):\n",
    "        return (\n",
    "            input_col \n",
    "            | \"Group and sum\" >> beam.CombinePerKey(sum)\n",
    "            | \"Filter account\" >> beam.Filter(filter_on_count)\n",
    "            | \"Regular Employee\" >> beam.Map(format_output)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "21b9ede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accounts, Marco, 31, Regular Employee.\n",
      "Accounts, Rebekah, 31, Regular Employee.\n",
      "Accounts, Itoe, 31, Regular Employee.\n",
      "Accounts, Edouard, 31, Regular Employee.\n",
      "Accounts, Kyle, 62, Regular Employee.\n",
      "Accounts, Kumiko, 31, Regular Employee.\n",
      "Accounts, Gaston, 31, Regular Employee.\n",
      "HR, Beryl, 62, Regular Employee.\n",
      "HR, Olga, 31, Regular Employee.\n",
      "HR, Leslie, 31, Regular Employee.\n",
      "HR, Mindy, 31, Regular Employee.\n",
      "HR, Vicky, 31, Regular Employee.\n",
      "HR, Richard, 31, Regular Employee.\n",
      "HR, Kirk, 31, Regular Employee.\n",
      "HR, Kaori, 31, Regular Employee.\n",
      "HR, Oscar, 31, Regular Employee.\n"
     ]
    }
   ],
   "source": [
    "p8 = beam.Pipeline()\n",
    "\n",
    "input_collection = (\n",
    "    p8\n",
    "    | \"Read from file\" >> beam.io.ReadFromText('dept_data.txt')\n",
    "    | \"Map transform based on ,\" >> beam.Map(lambda record: record.split(','))\n",
    ")\n",
    "\n",
    "account_count = (\n",
    "    input_collection\n",
    "    | \"Filtering based on 'accounts'\" >> beam.Filter(lambda record: record[3] == \"Accounts\")\n",
    "    | \"Map transform based on account record\" >> beam.Map(lambda record: (\"Accounts, \" + record[1],1))\n",
    "    | \"compostite account\" >> MyTransform()\n",
    "    | \"write to account\" >> beam.io.WriteToText('data/account')\n",
    ")\n",
    "\n",
    "hr_count = (\n",
    "    input_collection\n",
    "    | \"Filtering based on 'hr'\" >> beam.Filter(lambda record: record[3] == \"HR\")\n",
    "    | \"Map transform based on hr record\" >> beam.Map(lambda record: (\"HR, \" + record[1],1))\n",
    "    | \"compostite hr\" >> MyTransform()\n",
    "    | \"write to hr\" >> beam.io.WriteToText('data/hr')\n",
    ")\n",
    "\n",
    "\n",
    "p8.run()\n",
    "\n",
    "\n",
    "!{'head -n 20 data/account-00000-of-00001'}\n",
    "\n",
    "!{'head -n 20 data/hr-00000-of-00001'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19c011",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "945e615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_tuple(element):\n",
    "    element_row = element.split(',')\n",
    "    return (element_row[0], element_row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "32aa2086",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A transform with label \"[95]: Read from file\" already exists in the pipeline. To apply a transform with a specified label write pvalue | \"label\" >> transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m p8 \u001b[38;5;241m=\u001b[39m beam\u001b[38;5;241m.\u001b[39mPipeline()\n\u001b[1;32m      3\u001b[0m dep_rows \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     p8\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead from file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mReadFromText(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdept_data.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap transform based on ,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mMap(ret_tuple)\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m loc_rows \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mp8\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRead from file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReadFromText\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocation.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap transform based on ,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mMap(ret_tuple)\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# account_count = (\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     input_collection\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     | \"Filtering based on 'accounts'\" >> beam.Filter(lambda record: record[3] == \"Accounts\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     | \"write to hr\" >> beam.io.WriteToText('data/hr')\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     33\u001b[0m result \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     34\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: dep_rows, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: loc_rows}\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;241m|\u001b[39mbeam\u001b[38;5;241m.\u001b[39mCoGroupByKey()\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite to result\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mWriteToText(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/result\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/apache_beam/transforms/ptransform.py:1092\u001b[0m, in \u001b[0;36m_NamedPTransform.__ror__\u001b[0;34m(self, pvalueish, _unused)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ror__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pvalueish, _unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1092\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__ror__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpvalueish\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/apache_beam/transforms/ptransform.py:614\u001b[0m, in \u001b[0;36mPTransform.__ror__\u001b[0;34m(self, left, label)\u001b[0m\n\u001b[1;32m    612\u001b[0m pvalueish \u001b[38;5;241m=\u001b[39m _SetInputPValues()\u001b[38;5;241m.\u001b[39mvisit(pvalueish, replacements)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m p\n\u001b[0;32m--> 614\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpvalueish\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deferred:\n\u001b[1;32m    616\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/apache_beam/pipeline.py:666\u001b[0m, in \u001b[0;36mPipeline.apply\u001b[0;34m(self, transform, pvalueish, label)\u001b[0m\n\u001b[1;32m    664\u001b[0m old_label, transform\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mlabel, label\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 666\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpvalueish\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m   transform\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m old_label\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/apache_beam/pipeline.py:680\u001b[0m, in \u001b[0;36mPipeline.apply\u001b[0;34m(self, transform, pvalueish, label)\u001b[0m\n\u001b[1;32m    676\u001b[0m full_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    677\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_transform()\u001b[38;5;241m.\u001b[39mfull_label, label \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    678\u001b[0m      transform\u001b[38;5;241m.\u001b[39mlabel])\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplied_labels:\n\u001b[0;32m--> 680\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA transform with label \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m already exists in the pipeline. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    682\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo apply a transform with a specified label write \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    683\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpvalue | \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m >> transform\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m full_label)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplied_labels\u001b[38;5;241m.\u001b[39madd(full_label)\n\u001b[1;32m    686\u001b[0m pvalueish, inputs \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39m_extract_input_pvalues(pvalueish)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A transform with label \"[95]: Read from file\" already exists in the pipeline. To apply a transform with a specified label write pvalue | \"label\" >> transform"
     ]
    }
   ],
   "source": [
    "p8 = beam.Pipeline()\n",
    "\n",
    "dep_rows = (\n",
    "    p8\n",
    "    | \"Read from file\" >> beam.io.ReadFromText('dept_data.txt')\n",
    "    | \"Map transform1,\" >> beam.Map(ret_tuple)\n",
    ")\n",
    "\n",
    "loc_rows = (\n",
    "    p8\n",
    "    | \"Read from loc file\" >> beam.io.ReadFromText('location.txt')\n",
    "    | \"Map transform2,\" >> beam.Map(ret_tuple)\n",
    "\n",
    ")\n",
    "\n",
    "# account_count = (\n",
    "#     input_collection\n",
    "#     | \"Filtering based on 'accounts'\" >> beam.Filter(lambda record: record[3] == \"Accounts\")\n",
    "#     | \"Map transform based on account record\" >> beam.Map(lambda record: (\"Accounts, \" + record[1],1))\n",
    "#     | \"compostite account\" >> MyTransform()\n",
    "#     | \"write to account\" >> beam.io.WriteToText('data/account')\n",
    "# )\n",
    "\n",
    "# hr_count = (\n",
    "#     input_collection\n",
    "#     | \"Filtering based on 'hr'\" >> beam.Filter(lambda record: record[3] == \"HR\")\n",
    "#     | \"Map transform based on hr record\" >> beam.Map(lambda record: (\"HR, \" + record[1],1))\n",
    "#     | \"compostite hr\" >> MyTransform()\n",
    "#     | \"write to hr\" >> beam.io.WriteToText('data/hr')\n",
    "# )\n",
    "\n",
    "\n",
    "result = (\n",
    "    {\"dep_data\": dep_rows, \"loc_data\": loc_rows}\n",
    "    |beam.CoGroupByKey()\n",
    "    | \"write to result\" >> beam.io.WriteToText('data/result')\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "p8.run()\n",
    "\n",
    "!{'head -n 20 data/result-00000-of-00001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de3adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
